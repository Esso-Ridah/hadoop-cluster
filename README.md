# Analyse M√©t√©orologique avec Hadoop et Airflow

Ce projet permet d'analyser des donn√©es m√©t√©orologiques en utilisant Hadoop pour le traitement distribu√© et Airflow pour l'orchestration des t√¢ches. Il inclut des DAGs pour l'extraction, la transformation et le chargement des donn√©es m√©t√©orologiques.

## Architecture

Le projet utilise les composants suivants :
- **Hadoop** : Pour le stockage distribu√© (HDFS) et le traitement des donn√©es
- **Hive** : Pour l'analyse des donn√©es avec SQL
- **Airflow** : Pour l'orchestration des t√¢ches ETL
- **PostgreSQL** : Pour la base de donn√©es Airflow

## Pr√©requis

- Docker et Docker Compose
- Au moins 8GB de RAM disponible
- Au moins 20GB d'espace disque disponible
- Ports disponibles : 8080 (Airflow), 9870 (HDFS), 10000 (Hive)

## Installation

1. Cloner le d√©p√¥t :
```bash
git clone [URL_DU_REPO]
cd hadoop_weather_analysis
```

2. D√©marrer l'environnement :
```bash
# D√©marrer les services Hadoop et Hive
sudo docker-compose -f hadoop-cluster/docker-compose.yml up -d

# D√©marrer les services Airflow
sudo docker-compose -f hadoop-cluster/docker-compose.dev.yml up -d
```

## Structure du Projet

```
hadoop-cluster/
‚îú‚îÄ‚îÄ airflow/
‚îÇ   ‚îú‚îÄ‚îÄ dags/              # DAGs Airflow
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ example_etl_dag.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_hdfs_connection.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ test_hive_connection.py
‚îÇ   ‚îú‚îÄ‚îÄ logs/             # Logs Airflow
‚îÇ   ‚îî‚îÄ‚îÄ plugins/          # Plugins Airflow
‚îú‚îÄ‚îÄ hadoop/
‚îÇ   ‚îú‚îÄ‚îÄ namenode/        # Donn√©es HDFS namenode
‚îÇ   ‚îî‚îÄ‚îÄ datanode*/       # Donn√©es HDFS datanodes
‚îú‚îÄ‚îÄ hive/
‚îÇ   ‚îú‚îÄ‚îÄ metastore/       # M√©tadonn√©es Hive
‚îÇ   ‚îî‚îÄ‚îÄ warehouse/       # Donn√©es Hive
‚îú‚îÄ‚îÄ docker-compose.yml           # Configuration Hadoop et Hive
‚îî‚îÄ‚îÄ docker-compose.dev.yml       # Configuration Airflow
```

## DAGs Disponibles

1. **example_etl_dag.py**
   - T√¢che 1 : Extraction de donn√©es m√©t√©o et stockage dans HDFS
   - T√¢che 2 : Transformation des donn√©es et cr√©ation d'une table Hive
   - T√¢che 3 : Cr√©ation d'une vue agr√©g√©e des donn√©es

2. **test_hdfs_connection.py**
   - Test de la connexion √† HDFS
   - Cr√©ation d'un r√©pertoire test
   - V√©rification des permissions

3. **test_hive_connection.py**
   - Test de la connexion √† Hive
   - Cr√©ation d'une base de donn√©es test
   - Cr√©ation d'une table test

## Utilisation

1. **Acc√©der √† l'interface Airflow**
   - URL : http://localhost:8080
   - Identifiants : airflow/airflow

2. **V√©rifier les services**
   - HDFS Web UI : http://localhost:9870
   - Hive Server : localhost:10000

3. **Commandes utiles**
   ```bash
   # Voir les logs des services
   sudo docker logs airflow-web
   sudo docker logs airflow-scheduler
   sudo docker logs namenode
   sudo docker logs hive-server

   # Arr√™ter les services
   sudo docker-compose -f hadoop-cluster/docker-compose.yml down
   sudo docker-compose -f hadoop-cluster/docker-compose.dev.yml down
   ```

## D√©pannage

1. **Probl√®mes de connexion**
   - V√©rifier que tous les services sont en cours d'ex√©cution
   - V√©rifier les logs des services
   - S'assurer que les ports ne sont pas d√©j√† utilis√©s

2. **Probl√®mes avec les DAGs**
   - V√©rifier les logs Airflow
   - S'assurer que les connexions sont correctement configur√©es
   - V√©rifier les permissions HDFS

## Nettoyage

Pour arr√™ter et nettoyer l'environnement :
```bash
# Arr√™ter les services
sudo docker-compose -f hadoop-cluster/docker-compose.yml down
sudo docker-compose -f hadoop-cluster/docker-compose.dev.yml down

# Supprimer les volumes (optionnel)
sudo docker-compose -f hadoop-cluster/docker-compose.yml down -v
sudo docker-compose -f hadoop-cluster/docker-compose.dev.yml down -v
```

## Ressources

- [Documentation Airflow](https://airflow.apache.org/docs/)
- [Documentation Hadoop](https://hadoop.apache.org/docs/current/)
- [Documentation Hive](https://cwiki.apache.org/confluence/display/Hive/Home)

# Installation du Cluster Hadoop avec Pig, Mahout et Oozie

Ce guide explique comment installer et configurer un cluster Hadoop avec les services Pig, Mahout et Oozie.

## Pr√©requis

- Docker
- Docker Compose
- Au moins 8GB de RAM disponible
- Au moins 20GB d'espace disque disponible

## Installation

1. Clonez le d√©p√¥t :
```bash
git clone <url-du-repo>
cd hadoop-cluster
```

2. Rendez le script d'installation ex√©cutable :
```bash
chmod +x setup.sh
```

3. Ex√©cutez le script d'installation :
```bash
./setup.sh
```

Ce script va :
- Cr√©er les r√©pertoires n√©cessaires pour Pig, Mahout et Oozie
- G√©n√©rer les scripts d'installation pour chaque service
- Cr√©er des exemples de fichiers de configuration
- Rendre tous les scripts ex√©cutables

## D√©marrage des Services

1. D√©marrez tous les services avec Docker Compose :
```bash
docker-compose up -d
```

2. V√©rifiez que tous les services sont en cours d'ex√©cution :
```bash
docker-compose ps
```

3. Pour voir les logs des services :
```bash
docker-compose logs -f pig-server mahout-server oozie
```

## Structure des R√©pertoires

```
hadoop-cluster/
‚îú‚îÄ‚îÄ pig/
‚îÇ   ‚îú‚îÄ‚îÄ install.sh
‚îÇ   ‚îî‚îÄ‚îÄ scripts/
‚îÇ       ‚îî‚îÄ‚îÄ example.pig
‚îú‚îÄ‚îÄ mahout/
‚îÇ   ‚îú‚îÄ‚îÄ install.sh
‚îÇ   ‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ example.sh
‚îÇ   ‚îî‚îÄ‚îÄ data/
‚îú‚îÄ‚îÄ oozie/
‚îÇ   ‚îú‚îÄ‚îÄ install.sh
‚îÇ   ‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ conf/
‚îÇ   ‚îú‚îÄ‚îÄ logs/
‚îÇ   ‚îî‚îÄ‚îÄ workflows/
‚îÇ       ‚îî‚îÄ‚îÄ workflow.xml
‚îî‚îÄ‚îÄ docker-compose.yml
```

## Configuration des Services

### Pig
- Le script d'installation est dans `pig/install.sh`
- Les scripts Pig sont stock√©s dans `pig/scripts/`
- Un exemple de script est fourni dans `pig/scripts/example.pig`

### Mahout
- Le script d'installation est dans `mahout/install.sh`
- Les scripts Mahout sont stock√©s dans `mahout/scripts/`
- Les donn√©es sont stock√©es dans `mahout/data/`
- Un exemple de script est fourni dans `mahout/scripts/example.sh`

### Oozie
- Le script d'installation est dans `oozie/install.sh`
- Les workflows sont stock√©s dans `oozie/workflows/`
- Les logs sont dans `oozie/logs/`
- La configuration est dans `oozie/conf/`
- Un exemple de workflow est fourni dans `oozie/workflows/workflow.xml`

## D√©pannage

Si vous rencontrez des probl√®mes :

1. V√©rifiez les logs des services :
```bash
docker-compose logs -f <nom-du-service>
```

2. Red√©marrez un service sp√©cifique :
```bash
docker-compose restart <nom-du-service>
```

3. Reconstruire un service :
```bash
docker-compose up -d --build <nom-du-service>
```

## Arr√™t des Services

Pour arr√™ter tous les services :
```bash
docker-compose down
```

Pour arr√™ter et supprimer les volumes :
```bash
docker-compose down -v
```

## Notes Importantes

- Assurez-vous que HDFS est pr√™t avant d'utiliser Pig, Mahout ou Oozie
- Les services peuvent prendre plusieurs minutes pour d√©marrer compl√®tement
- Consultez les logs si un service ne d√©marre pas correctement
- Les donn√©es persistantes sont stock√©es dans les volumes Docker

# Guide d'Installation - Environnement Hadoop avec Airflow

## Pr√©requis

1. **Syst√®me d'exploitation**
   - Linux (Ubuntu 20.04 ou sup√©rieur recommand√©)
   - Au moins 8GB de RAM
   - Au moins 20GB d'espace disque libre

2. **Installation des d√©pendances**
   ```bash
   # Mise √† jour du syst√®me
   sudo apt update && sudo apt upgrade -y

   # Installation de Docker et Docker Compose
   sudo apt install -y docker.io docker-compose
   sudo systemctl enable docker
   sudo systemctl start docker
   sudo usermod -aG docker $USER
   ```

3. **Red√©marrage de session**
   - D√©connectez-vous et reconnectez-vous pour que les changements de groupe prennent effet

## Structure du Projet

1. **Cr√©ation du r√©pertoire de travail**
   ```bash
   mkdir -p ~/hadoop_weather_analysis
   cd ~/hadoop_weather_analysis
   ```

2. **Cloner le d√©p√¥t (√† adapter selon votre m√©thode de distribution)**
   ```bash
   # Si vous utilisez Git
   git clone <votre-repo> hadoop-cluster
   cd hadoop-cluster
   ```

## Configuration d'Airflow

1. **Structure des fichiers Airflow**
   ```
   hadoop-cluster/
   ‚îî‚îÄ‚îÄ airflow/
       ‚îú‚îÄ‚îÄ config/
       ‚îÇ   ‚îî‚îÄ‚îÄ connections.json
       ‚îú‚îÄ‚îÄ dags/
       ‚îÇ   ‚îî‚îÄ‚îÄ test_connections.py
       ‚îú‚îÄ‚îÄ scripts/
       ‚îÇ   ‚îî‚îÄ‚îÄ import_connections.py
       ‚îî‚îÄ‚îÄ Dockerfile
   ```

2. **Configuration des connexions**
   Cr√©ez le fichier `airflow/config/connections.json` :
   ```json
   [
     {
       "conn_id": "hdfs_default",
       "conn_type": "hdfs",
       "host": "namenode",
       "port": 8020,
       "login": "airflow",
       "extra": {
         "use_kerberos": false
       }
     },
     {
       "conn_id": "webhdfs_default",
       "conn_type": "webhdfs",
       "host": "namenode",
       "port": 9870,
       "login": "airflow",
       "extra": {
         "use_kerberos": false
       }
     },
     {
       "conn_id": "hive_default",
       "conn_type": "hive",
       "host": "hive-server",
       "port": 10000,
       "login": "airflow",
       "schema": "default"
     },
     {
       "conn_id": "spark_default",
       "conn_type": "spark",
       "host": "spark-master",
       "port": 7077,
       "extra": {
         "queue": "default"
       }
     }
   ]
   ```

3. **DAG de test**
   Cr√©ez le fichier `airflow/dags/test_connections.py` :
   ```python
   from airflow import DAG
   from airflow.providers.apache.hdfs.hooks.hdfs import HDFSHook
   from airflow.operators.python import PythonOperator
   from datetime import datetime, timedelta

   def check_hdfs_conn(**kwargs):
       hook = HDFSHook('hdfs_default')
       client = hook.get_conn()
       client.listStatus('/')  # l√®ve si KO

   default_args = {
       'owner': 'airflow',
       'retries': 1,
       'retry_delay': timedelta(minutes=5),
       'start_date': datetime(2025,1,1),
   }

   with DAG('test_hdfs_connection',
            default_args=default_args,
            schedule_interval=None,
            catchup=False) as dag:

       t1 = PythonOperator(
           task_id='check_hdfs',
           python_callable=check_hdfs_conn,
           provide_context=True,
       )
   ```

4. **Script d'import des connexions**
   Cr√©ez le fichier `airflow/scripts/import_connections.py` :
   ```python
   #!/usr/bin/env python3
   import json
   import os
   from airflow import settings
   from airflow.models import Connection
   from sqlalchemy.orm import Session

   def import_connections():
       config_dir = os.getenv('AIRFLOW_HOME', '/opt/airflow') + '/config'
       path = os.path.join(config_dir, 'connections.json')
       if not os.path.isfile(path):
           print(f"‚ö†Ô∏è  Fichier non trouv√© : {path}")
           return

       with open(path) as f:
           data = json.load(f)

       if isinstance(data, dict):
           connections = list(data.values())
       elif isinstance(data, list):
           connections = data
       else:
           raise ValueError("Le fichier connections.json doit √™tre un list ou un dict racine")

       session: Session = settings.Session()
       for conn in connections:
           cid = conn.get('conn_id')
           if not cid:
               continue
           existing = session.query(Connection).filter_by(conn_id=cid).first()
           if existing:
               print(f"üîÑ Update {cid}")
               for attr in ('conn_type','host','login','password','schema','port'):
                   if conn.get(attr) is not None:
                       setattr(existing, attr, conn[attr])
               if conn.get('extra') is not None:
                   existing.set_extra(conn['extra'])
           else:
               print(f"‚ûï Create {cid}")
               new = Connection(
                   conn_id=cid,
                   conn_type=conn.get('conn_type'),
                   host=conn.get('host'),
                   login=conn.get('login'),
                   password=conn.get('password'),
                   schema=conn.get('schema'),
                   port=conn.get('port'),
               )
               if conn.get('extra'):
                   new.set_extra(conn['extra'])
               session.add(new)
       session.commit()
       session.close()
       print("‚úÖ Import termin√©")

   if __name__ == "__main__":
       import_connections()
   ```

## D√©marrage des Services

1. **Lancement d'Airflow**
   ```bash
   # Arr√™t des services existants
   docker-compose -f docker-compose.dev.yml down

   # Construction de l'image
   docker-compose -f docker-compose.dev.yml build --no-cache airflow-web

   # D√©marrage du service
   docker-compose -f docker-compose.dev.yml up -d airflow-web

   # V√©rification des logs
   docker-compose -f docker-compose.dev.yml logs -f airflow-web
   ```

2. **V√©rification**
   - Attendez que les logs indiquent que le service est pr√™t
   - V√©rifiez que vous voyez les messages de mise √† jour des connexions
   - Le webserver devrait √™tre accessible sur http://localhost:8080

## D√©pannage

1. **Probl√®mes courants**
   - Si les connexions ne sont pas import√©es, v√©rifiez le format du fichier `connections.json`
   - Si le DAG n'appara√Æt pas, v√©rifiez les logs pour les erreurs d'importation
   - Si le service ne d√©marre pas, v√©rifiez les logs pour les erreurs de configuration

2. **Commandes utiles**
   ```bash
   # Voir les logs en temps r√©el
   docker-compose -f docker-compose.dev.yml logs -f airflow-web

   # Red√©marrer le service
   docker-compose -f docker-compose.dev.yml restart airflow-web

   # Voir les conteneurs en cours d'ex√©cution
   docker-compose -f docker-compose.dev.yml ps
   ```

## Prochaines √©tapes

Une fois que tout est install√© et fonctionnel, nous pourrons :
1. Cr√©er l'utilisateur admin Airflow
2. Lancer les autres services (Hadoop, Hive, etc.)
3. Commencer les TP pratiques 

# TP Hadoop & Airflow - Analyse de Donn√©es M√©t√©o

Ce projet met en place un environnement de traitement de donn√©es avec Hadoop, Hive et Airflow pour l'analyse de donn√©es m√©t√©orologiques.

## Architecture

- **Hadoop** : Stockage distribu√© (HDFS) et traitement (MapReduce)
- **Hive** : Data warehouse pour requ√™tes SQL
- **Airflow** : Orchestration des workflows
- **PostgreSQL** : Base de donn√©es pour Airflow

## Pr√©requis

- Docker et Docker Compose
- Git
- Au moins 4GB de RAM disponible
- Ports 8080, 9870, 10000 disponibles

## Installation

1. Cloner le repository :
```bash
git clone [URL_DU_REPO]
cd hadoop_weather_analysis
```

2. D√©marrer l'environnement :
```bash
cd hadoop-cluster
docker-compose -f docker-compose.dev.yml up -d
```

3. Attendre que tous les services soient d√©marr√©s (environ 2-3 minutes)

4. Acc√©der aux interfaces :
   - Airflow : http://localhost:8080 (user: airflow, password: airflow)
   - HDFS : http://localhost:9870
   - Hive : via beeline (port 10000)

## Structure du Projet

```
hadoop_weather_analysis/
‚îú‚îÄ‚îÄ hadoop-cluster/
‚îÇ   ‚îú‚îÄ‚îÄ docker-compose.dev.yml    # Configuration Docker
‚îÇ   ‚îú‚îÄ‚îÄ airflow/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ dags/                 # DAGs Airflow
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ example_etl_dag.py    # DAG principal
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ test_hdfs_connection.py
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ test_hive_connection.py
‚îÇ   ‚îî‚îÄ‚îÄ scripts/                  # Scripts utilitaires
‚îî‚îÄ‚îÄ README.md
```

## DAGs Disponibles

1. **example_etl_dag.py**
   - Processus ETL complet
   - 3 t√¢ches : extraction, transformation, chargement
   - Utilise HDFS et Hive

2. **test_hdfs_connection.py**
   - Test de connexion √† HDFS
   - Cr√©ation de r√©pertoires et fichiers

3. **test_hive_connection.py**
   - Test de connexion √† Hive
   - Cr√©ation de base de donn√©es et tables

## Utilisation

### 1. V√©rifier les Services

```bash
docker ps
```
V√©rifier que tous les services sont en √©tat "Up" :
- namenode
- datanode
- hive-server
- airflow-web
- airflow-scheduler
- postgres

### 2. Tester les Connexions

cr√©er un user: 
```bash
docker exec -it airflow-web airflow users create --username admin --firstname Admin --lastname User --role Admin --email admin@example.com --password admin
```
1. Dans l'interface Airflow (http://localhost:8080) :
   - Activer le DAG `test_hdfs_connection`
   - D√©clencher une ex√©cution manuelle
   - V√©rifier les logs

2. R√©p√©ter avec `test_hive_connection`

### 3. Ex√©cuter le DAG Principal

1. Activer `example_etl_dag`
2. D√©clencher une ex√©cution
3. Suivre l'ex√©cution dans l'interface
4. V√©rifier les r√©sultats dans Hive

## Commandes Utiles

### HDFS
```bash
# Lister les fichiers
docker exec namenode hdfs dfs -ls /user/airflow

# Voir le contenu d'un fichier
docker exec namenode hdfs dfs -cat /user/airflow/weather_data/*
```

### Hive
```bash
# Se connecter √† Hive
docker exec -it hive-server beeline -u jdbc:hive2://localhost:10000 -n airflow

# Requ√™tes utiles
SHOW DATABASES;
USE weather_db;
SHOW TABLES;
SELECT * FROM weather_summary;
```

### Airflow
```bash
# Voir les logs d'un DAG
docker logs airflow-scheduler

# Red√©marrer un service
docker-compose -f docker-compose.dev.yml restart airflow-web
```

## D√©pannage

1. **Airflow ne d√©marre pas**
   - V√©rifier les logs : `docker logs airflow-web`
   - Red√©marrer : `docker-compose -f docker-compose.dev.yml restart airflow-web`

2. **HDFS inaccessible**
   - V√©rifier que namenode est en cours d'ex√©cution
   - V√©rifier les logs : `docker logs namenode`

3. **Hive ne r√©pond pas**
   - V√©rifier que hive-server est en cours d'ex√©cution
   - V√©rifier les logs : `docker logs hive-server`

## Nettoyage

Pour arr√™ter et nettoyer l'environnement :
```bash
docker-compose -f docker-compose.dev.yml down -v
```

## Ressources Additionnelles

- [Documentation Airflow](https://airflow.apache.org/docs/)
- [Documentation Hive](https://cwiki.apache.org/confluence/display/Hive/Home)
- [Documentation HDFS](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsUserGuide.html) 
